{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vapemx/AeroFeel/blob/main/AeroFeel.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# AeroFeel\n",
        "###### by: Datahacks\n",
        "-----------------------\n",
        "\n",
        "El mercado aéreo comercial de México es altamente competitivo, lo que hace que la retención de clientes sea un aspecto crucial para alcanzar el éxito en este sector.\n",
        "\n",
        "## Reto\n",
        "\n",
        "VivaAerobus nos entregó un dataset en formato xlsx con 4 hojas, las cuáles están separadas por el punto en el que se recabó la opinión del cliente (paypoint). Este dataset con aproximadamente 76 mil registros en total, fue utilizado para el entrenamiento y evaluación de un modelo NLP, sin embargo, este notebook está creado con el objetivo de introducir la información que se recaba día con día. \n",
        "\n",
        "## Solución\n",
        "\n",
        "Para la solución del reto decidimos utiizar DistilBERT, que es una versión más ligera y rápida del modelo original BERT, el cuuál es un modelo de procesamiento del lenguaje natural. \n",
        "\n",
        "Utilizamos este modelo para un análisis de sentimiento, pero, para realizar el fine-tuning, comparamos la calificación dada por el cliente con el sentimiento dado por el modelo, para después compararlo en una matriz de confusión.\n",
        "\n",
        "Posteriormente, los comentarios se clasifican en las diferentes áreas de interés para la aerolínea con base en palabras clave; finalmente, se hace una limpieza de palabras altisonantes.\n",
        "\n",
        "## Resultados\n",
        "\n",
        "Con un promedio de 83% de precisión en los conjuntos de validación, tenemos un análisis de sentimiento satisfactorio gracias a nuestro modelo.\n",
        "\n",
        "Para este punto, en el dataset final ya tenemos el paypoint, sentimiento y clasificacón, así como todos los otros campos previamente dados. \n",
        "\n",
        "Con esta información, se le permite a la aerolínea personalizar la búsqueda de comentarios ya sea por área, sentimiento, clasificación y hasta rango de fechas, con el objetivo de enfocarse específicamente en las áreas de oportunidad."
      ],
      "metadata": {
        "id": "-9V7XdABIX-P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports e installs"
      ],
      "metadata": {
        "id": "26qvuZ1woFn1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets"
      ],
      "metadata": {
        "id": "j_Ym-qJ2d6Np"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "id": "gkvSQuqpd7Jf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sentencepiece"
      ],
      "metadata": {
        "id": "uQOeEU1Hd_A9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from datasets import Dataset\n",
        "import matplotlib.pyplot as plt\n",
        "from transformers import pipeline\n",
        "from torch.utils.data import Dataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
        "from transformers import DistilBertTokenizerFast, DistilBertForSequenceClassification, Trainer, TrainingArguments, AutoModelForSequenceClassification"
      ],
      "metadata": {
        "id": "KrUz9fqteBoT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Habilitación de aceleración por GPU\n",
        "\n",
        "**Nota: habilitar la aceleración por GPU de google colab**"
      ],
      "metadata": {
        "id": "sxvj_9cAeLQN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "config = tf.compat.v1.ConfigProto()\n",
        "config.gpu_options.allow_growth = True\n",
        "sess = tf.compat.v1.Session(config=config)"
      ],
      "metadata": {
        "id": "2oEqqo6IeKvK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Funciones de limpieza\n",
        "1. Identificar columnas vacias y borralas.\n",
        "2. Contar registros vacios.\n",
        "3. Summary de cuantos datos hay en cada columna.\n",
        "4. Renombrar columnas."
      ],
      "metadata": {
        "id": "ijA8jlsvoIiY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def changeDateWithoutHours(df, nameColumn='Date'):\n",
        "    df[nameColumn] = pd.to_datetime(df[nameColumn])\n",
        "    df[nameColumn] = df[nameColumn].dt.date\n",
        "    return df\n"
      ],
      "metadata": {
        "id": "wKdiWBfAoLqt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(df.info())"
      ],
      "metadata": {
        "id": "jBKQx3kHprBR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def searchNullColumns(df):\n",
        "  nullColumns = df.columns[df.isnull().any()].tolist()\n",
        "  if len(nullColumns) > 0:\n",
        "      print(\"Se encontraron columnas con filas vacias\")\n",
        "      return nullColumns\n",
        "  else:\n",
        "      print(\"No se encontraron columnas con valores vacios en el dataset.\")\n",
        "      return None\n"
      ],
      "metadata": {
        "id": "vaUHU699pxhX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def cleanColumns(df,columns):\n",
        "  df = df.drop(columns, axis=1)\n"
      ],
      "metadata": {
        "id": "JDWNnjUcqI_X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.info()"
      ],
      "metadata": {
        "id": "jvLSj1O4sAMz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import xlsx dataset"
      ],
      "metadata": {
        "id": "YIDj9tSasH6S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input = input(\"Ingrese la ruta del archivo xlsx: \")\n",
        "\n",
        "df_booking = pd.read_excel(input, sheet_name=\"Booking flow\")\n",
        "df_checkin = pd.read_excel(input, sheet_name=\"Checkin\")\n",
        "df_rsv = pd.read_excel(input, sheet_name=\"Manage my booking\")\n",
        "df_feedback = pd.read_excel(input, sheet_name=\"Feedback button\")"
      ],
      "metadata": {
        "id": "1bRWwv4msIqb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Clean and analyze data"
      ],
      "metadata": {
        "id": "ZMNYbn94gP5p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_checkin['Comment'].fillna(\"N/A\", inplace=True)"
      ],
      "metadata": {
        "id": "dNz1N9wesKgY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_booking = pd.read_csv(booking)\n",
        "df_booking.head()\n",
        "df_booking = changeDateWithoutHours(df_booking)"
      ],
      "metadata": {
        "id": "rFFtsyinOruj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nullCol_booking= searchNullColumns(df_booking)\n",
        "fNull_booking = df_booking[\"text\"].isnull().sum()\n",
        "fNull_booking"
      ],
      "metadata": {
        "id": "GuK0mONLLxvL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_booking[\"text\"] = df_booking[\"text\"].fillna(\"NA\")"
      ],
      "metadata": {
        "id": "h6r2dQ-cLzhW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_booking"
      ],
      "metadata": {
        "id": "LVmbAlUGL1Tt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_rsv = pd.read_csv(rsv)\n",
        "df_rsv.head()"
      ],
      "metadata": {
        "id": "-dTMEz_wNYGt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_rsv = changeDateWithoutHours(df_rsv)\n",
        "nullCol_rsv= searchNullColumns(df_rsv)\n",
        "fNull_rsv = df_rsv[\"_C_mo_podemos_mejorar_\"].isnull().sum()\n",
        "fNull_rsv"
      ],
      "metadata": {
        "id": "bMjs5in3No3T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_rsv[\"_C_mo_podemos_mejorar_\"] = df_rsv[\"_C_mo_podemos_mejorar_\"].fillna(\"NA\")"
      ],
      "metadata": {
        "id": "fJzZhoFCNqPS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_booking[\"nps_scaled\"] = df_booking[\"nps\"]//2"
      ],
      "metadata": {
        "id": "tBfn9corXFTb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_rsv[\"nps_scaled\"] = df_rsv[\"nps\"]//2"
      ],
      "metadata": {
        "id": "Th7eH2EuXCML"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cleaning feedback dataframe\n",
        "\n",
        "La tabla \"Feedback button\" cuenta con una estructura completamente diferente a las otras, por lo que se necesita limpiar y ordenar de forma especial."
      ],
      "metadata": {
        "id": "qar2DBa6vx4X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_feedback.head()"
      ],
      "metadata": {
        "id": "KcYIn7Nbv5YY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.bar(df_feedback.columns, df_feedback.count())\n",
        "plt.ylabel('Cantidad de Registros')\n",
        "plt.xticks(rotation = 90)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "oasfMz3pvvLf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_feedback = df_feedback.dropna(axis=1, how='all')\n",
        "# Eliminamos columnas con menos de 10 registros\n",
        "df_feedback = df_feedback.loc[:, df_feedback.notnull().sum() >= 1000]\n",
        "\n",
        "#Eliminamos una columna de relleno\n",
        "df_feedback.drop('Image', axis=1, inplace=True)\n",
        "\n",
        "#Renombramos y modificamos valores de las columnas para evitar registros nulos\n",
        "df_feedback = df_feedback.rename(columns={'Unnamed: 2': 'Comments', 'Comment': 'Date'})\n",
        "df_feedback['Page Load Time'] = df_feedback['Page Load Time'].str.replace('s', '')\n",
        "df_feedback['Page Load Time'] = df_feedback['Page Load Time'].fillna(value=3000)\n",
        "df_feedback['Page Load Time'] = df_feedback['Page Load Time'].astype(int)\n",
        "df_feedback['Page Load Time'] = df_feedback['Page Load Time'].replace(3000, np.nan)\n",
        "df_feedback['Comments'].fillna('Ninguno', inplace=True)\n",
        "df_feedback['Page Load Time'].fillna(df_feedback['Page Load Time'].mean(), inplace=True)\n",
        "df_feedback['feedback_type'].fillna('Unknown', inplace=True)\n",
        "df_feedback['basket_id'] = df_feedback['basket_id'].fillna(value=False).astype(bool)\n",
        "df_feedback['¿Qué te gustaría compartir con nosotros?'].fillna('Nada', inplace=True)\n",
        "df_feedback['motivo_visita'].fillna('Unknown', inplace=True)\n",
        "df_feedback['share_data'].fillna('no', inplace=True)\n",
        "df_feedback['share_data'].replace(['si', 'yes'], True, inplace=True)\n",
        "df_feedback['share_data'].replace('no', False, inplace=True)\n",
        "df_feedback['error_type'].fillna('Unknown', inplace=True)\n",
        "df_feedback.head(5)\n",
        "\n",
        "\n",
        "df_feedback['Rating'].value_counts()\n",
        "df_feedback = df_feedback.rename(columns={'Rating':'nps_scaled'})\n",
        "df_feedback.isnull().sum()"
      ],
      "metadata": {
        "id": "I2utCg0Gv7tr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Análisis de sentimiento"
      ],
      "metadata": {
        "id": "smvG49RRlBOO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def analyze(df):\n",
        "  # Dividir el conjunto de datos en entrenamiento y validación\n",
        "  train, val = train_test_split(df, test_size=0.2, random_state=42)\n",
        "\n",
        "  # Inicializa el tokenizador\n",
        "  tokenizer = DistilBertTokenizerFast.from_pretrained(\"distilbert-base-multilingual-cased\")\n",
        "\n",
        "  # Tokenizar el texto\n",
        "  train_encodings = tokenizer(list(train['Comments']), truncation=True, padding=True)\n",
        "  val_encodings = tokenizer(list(val['Comments']), truncation=True, padding=True)\n",
        "\n",
        "  # Cargar el pipeline de análisis de sentimiento\n",
        "  sentiment_analysis = pipeline(\n",
        "      \"sentiment-analysis\",\n",
        "      model=\"nlptown/bert-base-multilingual-uncased-sentiment\",\n",
        "      tokenizer=\"nlptown/bert-base-multilingual-uncased-sentiment\",\n",
        "  )\n",
        "\n",
        "  # Función para predecir el sentimiento de un comentario\n",
        "  def predict_sentiment(comment):\n",
        "      # Truncar el comentario si excede 512 tokens\n",
        "      truncated_comment = comment[:512]\n",
        "      \n",
        "      result = sentiment_analysis(truncated_comment)\n",
        "      sentiment = result[0]['label'].split('_')[-1].lower()\n",
        "      return sentiment\n",
        "\n",
        "  # Clasifica los comentarios en la columna 'Comments'\n",
        "  df['Sentiment'] = df['Comments'].apply(predict_sentiment)\n",
        "\n",
        "  class FeedbackDataset(Dataset):\n",
        "    def __init__(self, encodings, labels):\n",
        "        self.encodings = encodings\n",
        "        self.labels = labels\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
        "        item['labels'] = torch.tensor(self.labels[idx])\n",
        "        return item\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "  # Crear los conjuntos de datos de entrenamiento y validación\n",
        "  train_labels = train['Comments'].map({'positive': 0, 'negative': 1, 'neutral': 2}).tolist()\n",
        "  val_labels = val['Comments'].map({'positive': 0, 'negative': 1, 'neutral': 2}).tolist()\n",
        "  train_dataset = FeedbackDataset(train_encodings, train_labels)\n",
        "  val_dataset = FeedbackDataset(val_encodings, val_labels)\n",
        "\n",
        "  import pandas as pd\n",
        "  from datasets import Dataset\n",
        "  from transformers import DistilBertTokenizerFast, DistilBertForSequenceClassification\n",
        "  from transformers import Trainer, TrainingArguments\n",
        "  from sklearn.metrics import accuracy_score\n",
        "\n",
        "  # 1. Cargar y preparar los datos\n",
        "  # Suponiendo que ya tienes un dataframe 'df' con las columnas 'Comments' y 'Sentiment'\n",
        "\n",
        "  # Renombrar las columnas\n",
        "  df = df.rename(columns={'Comments': 'text', 'Sentiment': 'label'})\n",
        "\n",
        "  # Crear un diccionario para convertir las etiquetas a números\n",
        "  label_to_id = {\n",
        "      '1 star': 0, \n",
        "      '2 stars': 1, \n",
        "      '3 stars': 2,\n",
        "      '4 stars': 3,\n",
        "      '5 stars': 4\n",
        "  }\n",
        "  df['label'] = df['label'].apply(lambda x: label_to_id[x])\n",
        "\n",
        "  # Dividir los datos en conjuntos de entrenamiento y validación (ajusta la proporción según lo necesario)\n",
        "  train_df = df.sample(frac=0.8, random_state=42)\n",
        "  val_df = df.drop(train_df.index)\n",
        "\n",
        "  # Convertir los dataframes de pandas a datasets de Hugging Face\n",
        "  train_dataset = Dataset.from_pandas(train_df)\n",
        "  val_dataset = Dataset.from_pandas(val_df)\n",
        "\n",
        "  # 2. Preparar el tokenizador y el modelo\n",
        "  tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-multilingual-cased')\n",
        "  model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-multilingual-cased', num_labels=5)\n",
        "\n",
        "  # 3. Tokenizar y codificar los conjuntos de datos\n",
        "  max_length = 128\n",
        "\n",
        "  def encode_examples(examples):\n",
        "      encoded = tokenizer(\n",
        "          examples['text'], \n",
        "          truncation=True, \n",
        "          padding='max_length', \n",
        "          max_length=max_length\n",
        "      )\n",
        "      \n",
        "      labels = examples['label']\n",
        "      encoded.update({'labels': labels})\n",
        "      \n",
        "      return encoded\n",
        "\n",
        "  train_dataset = train_dataset.map(encode_examples, batched=True)\n",
        "  val_dataset = val_dataset.map(encode_examples, batched=True)\n",
        "\n",
        "  # 4. Configurar y entrenar el Trainer\n",
        "  def compute_metrics(pred):\n",
        "      labels = pred.label_ids\n",
        "      preds = pred.predictions.argmax(-1)\n",
        "      acc = accuracy_score(labels, preds)\n",
        "      return {'accuracy': acc}\n",
        "\n",
        "  training_args = TrainingArguments(\n",
        "      output_dir='./results',\n",
        "      num_train_epochs=3,\n",
        "      per_device_train_batch_size=16,\n",
        "      per_device_eval_batch_size=16,\n",
        "      logging_dir='./logs',\n",
        "      logging_steps=10,\n",
        "      evaluation_strategy=\"steps\",\n",
        "      save_strategy=\"steps\",\n",
        "      save_steps=50,\n",
        "      load_best_model_at_end=True,\n",
        "      metric_for_best_model=\"accuracy\",\n",
        "      seed=42,\n",
        "  )\n",
        "\n",
        "  trainer = Trainer(\n",
        "      model=model,\n",
        "      args=training_args,\n",
        "      train_dataset=train_dataset,\n",
        "      eval_dataset=val_dataset,\n",
        "      compute_metrics=compute_metrics,\n",
        "  )\n",
        "\n",
        "\n",
        "  trainer.train()\n",
        "\n",
        "\n",
        "  # Guardar el modelo entrenado y el tokenizador\n",
        "  trainer.save_model(\"sentiment_analysis_multilingual\")\n",
        "  tokenizer.save_pretrained(\"sentiment_analysis_multilingual\")\n",
        "\n",
        "  # Evaluar el modelo en el conjunto de datos de entrenamiento\n",
        "  train_eval_results = trainer.evaluate(train_dataset)\n",
        "\n",
        "  print(\"Resultados de la evaluación en el conjunto de entrenamiento de:\")\n",
        "  print(train_eval_results)\n",
        "\n",
        "  # Evaluar el modelo en el conjunto de datos de validación\n",
        "  eval_results = trainer.evaluate()\n",
        "\n",
        "  print(\"Resultados de la evaluación en el conjunto de validación:\")\n",
        "  print(eval_results)\n",
        "\n",
        "  def custom_sentiment_pipeline(text, model, tokenizer, negative_words):\n",
        "    # Verificar si alguna palabra negativa está presente en el texto\n",
        "    if any(word.lower() in text.lower() for word in negative_words):\n",
        "        return \"1 star\"\n",
        "\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True)\n",
        "    outputs = model(**inputs)\n",
        "    predictions = outputs.logits.argmax(dim=-1).item()\n",
        "\n",
        "    return id_to_label[predictions]\n",
        "\n",
        "  # Cargar el modelo afinado y el tokenizador\n",
        "  model = AutoModelForSequenceClassification.from_pretrained(\"sentiment_analysis_multilingual\")\n",
        "  tokenizer = DistilBertTokenizerFast.from_pretrained(\"sentiment_analysis_multilingual\")\n",
        "\n",
        "  # Diccionario de conversiones de ID a etiqueta\n",
        "  id_to_label = {0: 1, 1: 2, 2: 3, 3: 4, 4: 5}\n",
        "\n",
        "  # Lista de palabras negativas\n",
        "  negative_words = [\"malo\", \"terrible\", \"horrible\", \"pésimo\", \"asqueroso\", \"chinguen\", \"pendejos\"]\n",
        "\n",
        "  # Comentario de ejemplo en español\n",
        "  comentario = \"horrible\"\n",
        "\n",
        "  # Probar el pipeline con el comentario de ejemplo\n",
        "  sentiment_label = custom_sentiment_pipeline(comentario, model, tokenizer, negative_words)\n",
        "\n",
        "  print(f\"Comentario: {comentario}\")\n",
        "  print(f\"Sentimiento: {sentiment_label}\")\n",
        "\n",
        "  return df"
      ],
      "metadata": {
        "id": "8hUAdeUGlC04"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Get Predicted Data"
      ],
      "metadata": {
        "id": "Mk11GfQqnboV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chekin = analyze(df_checkin)\n",
        "feedback = analyze(df_feedback)\n",
        "rsv = analyze(df_rsv)\n",
        "booking = analyze(df_booking)"
      ],
      "metadata": {
        "id": "6xQZzOJwnBWS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Categorize data"
      ],
      "metadata": {
        "id": "k5MnmupynkBG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "checkin['paypoint'] = 'checkin'\n",
        "feedback['paypoint'] = 'feedback'\n",
        "rsv['paypoint'] = 'rsv'\n",
        "booking['paypoint'] = 'booking'"
      ],
      "metadata": {
        "id": "0YWHSYdbnqgt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = pd.concat([checkin, feedback, rsv, booking], ignore_index=True)\n",
        "dataset = dataset.fillna('None')"
      ],
      "metadata": {
        "id": "rvQiLLTnnyiS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Diferentes categorías y palabras clave que nos pueden interesar para clasificar los comentarios\n",
        "\n",
        "categories = {\n",
        "    'Pagos': ['confirmacion', 'cancelacion', 'pago', 'pagos', 'tarjeta', 'banco', 'credito', 'debito', 'TDC', 'tdc', 'rechazo', 'declinado',\n",
        "              'Boleto', 'Pasaje', 'Asiento', 'Ventanilla', 'Pasillo', 'Facturación', 'Equipaje', 'Sobrepeso', 'Maleta', 'Mochila', \n",
        "              'Bolsa de viaje', 'Pago', 'Efectivo', 'Tarjeta de crédito', 'Tarjeta de débito', 'Transferencia bancaria', 'PayPal', 'Google Pay',\n",
        "              'Apple Pay', 'Moneda local', 'Cambio de moneda', 'Tasa de cambio', 'Impuestos', 'Seguro de viaje', 'Seguro de cancelación',\n",
        "              'Seguro de equipaje', 'Seguro médico', 'Descuento', 'Oferta', 'Promoción', 'Cupón', 'Código de descuento', 'Equipaje perdido', 'Carga',\n",
        "              'Tarifa de carga', 'Espacio de carga'],\n",
        "\n",
        "    'Precios': ['caro', 'costoso', 'asientos', 'tarifas', 'tarifa', 'precio', 'precios', 'equipaje', 'peso', 'sobrepeso', 'Asiento', 'Ventanilla',\n",
        "                'Pasillo', 'Equipaje', 'Sobrepeso', 'Maleta', 'Mochila', 'Bolsa de viaje', 'Tarifa de uso del aeropuerto', 'tua', 'barato', 'extras'],\n",
        "\n",
        "    'Técnicos': ['errores', 'carga', 'traba', 'trabado', 'trabada', 'idioma'], # Web\n",
        "    \n",
        "    'Operaciones': ['conexion', 'escala', 'escalas', 'vuelo', 'vuelos', 'retraso', 'retrasos', 'cancelacion', 'cancelaciones',\n",
        "                    'cancelado', 'cancelada', 'cancelados', 'canceladas', 'demora', 'demoras', 'demorado', 'demorada', 'demorados',\n",
        "                    'demoradas', 'demorar', 'demore', 'demoren', 'accesibilidad'],\n",
        "\n",
        "    'Reservacion': ['cambios', 'reserva', 'asiento', 'asientos', 'reasignacion', 'cambio', 'Clase', 'Asiento', 'Ventanilla', 'Pasillo',  'Equipaje',\n",
        "                    'Sobrepeso', 'Maleta', 'Mochila', 'Bolsa de viaje', 'Confirmación de reserva', 'Cancelación de reserva', 'Cambio de reserva', 'Reembolso',\n",
        "                    'Política de cancelación', 'Política de cambio', 'Política de reembolso', 'Política de equipaje', 'Tamaño del equipaje', 'Peso del equipaje',\n",
        "                    'Equipaje de mano', 'Equipaje documentado', 'mascota'],\n",
        "\n",
        "    'Personal': ['Desinterés', 'Impaciencia', 'Arrogancia', 'Desgano', 'Indiferencia', 'Hostilidad', 'Desorganización', 'Indolencia', 'Antipatía',\n",
        "                 'Insolencia', 'Descortesía', 'Falta de empatía', 'Impuntualidad', 'Intransigencia', 'Falta de compromiso', 'Despreocupación', 'Indecisión',\n",
        "                 'Falta de cooperación', 'Desprestigio', 'Falta de comunicación', 'Inflexibilidad', 'Desconfianza', 'Deshonestidad', 'Falta de iniciativa',\n",
        "                 'Insensibilidad', 'Prepotencia', 'Desconcentración', 'Inatención', 'Desmotivación', 'Falta de respeto', 'Desconocimiento del trabajo',\n",
        "                 'Falta de ética', 'Falta de entusiasmo', 'Falta de paciencia', 'Falta de sinceridad', 'Inflexibilidad en políticas',\n",
        "                 'Incapacidad para resolver problemas', 'Falta de soluciones', 'Mala actitud hacia el trabajo', 'Falta de atención en detalles',\n",
        "                 'Desorden en tareas', 'Falta de habilidad para trabajar bajo presión', 'Falta de capacidad de trabajo en equipo', 'Desinteresado',\n",
        "                 'Impuntual', 'Descortés', 'Incompetente', 'Desorganizado', 'Desatento', 'Arrogante', 'Descuidado', 'Maleducado', 'Insensible', 'Irresponsable',\n",
        "                 'Inflexible', 'Indiferente', 'Indolente', 'Despreocupado', 'Desmotivado', 'Despreparado', 'Malhumorado', 'Desagradable', 'Indeciso',\n",
        "                 'Desconsiderado', 'Desleal', 'Insuficiente', 'Incumplido', 'Irrespetuoso', 'Desesperado', 'Deshonesto', 'Desaliñado', 'Desmotivante',\n",
        "                 'Desmotivador', 'Indiscreto', 'Indignado', 'Desenfocado', 'Desatendido', 'Desentendido', 'Despistado', 'Desaprensivo', 'Desobligante',\n",
        "                 'Descarado', 'Deshonrado', 'Desobediente', 'Atento', 'Cortés', 'Amable', 'Empático', 'Servicial', 'Proactivo', 'Colaborativo', 'Responsable',\n",
        "                 'Detallista', 'Preciso', 'Consciente', 'Creativo', 'Disponible', 'Espontáneo', 'Generoso', 'Innovador', 'Interesado', 'Inspirador', 'Leal',\n",
        "                 'Seguro', 'Tranquilo', 'Orientado al cliente', 'Positivo', 'Optimista', 'Emprendedor', 'Reflexivo', 'Solidario', 'Comunicativo', 'Trabajador',\n",
        "                 'Confiado', 'Visionario', 'Dinámico', 'Persistente', 'Flexible', 'Convincente', 'Confiable', 'Alegre', 'Comprensivo', 'Paciente', 'Eficiente',\n",
        "                 'Profesional', 'Sincero', 'Agradable', 'Respetuoso', 'Entusiasta', 'Organizado', 'Innovador', 'Interesado', 'Inspirador', 'Solidario'],\n",
        "\n",
        "    'Aeropuerto': ['aeropuerto', 'seguridad', 'Transporte de equipaje', 'perdida', 'Equipaje retrasado', 'control', 'aduana', 'inmigracion', 'migracion', 'sala de espera', 'sala de abordar', 'capilla'],\n",
        "\n",
        "    'Servicios': ['comida', 'a bordo', 'servicio',  'Wi-Fi', 'wifi', 'internet', 'bebidas', 'refrescos', 'jugos', 'fria', 'caliente', 'sabor', 'rica', 'mala', 'baños', 'luces', 'enchufes', 'enchufe', 'pantalla', 'entretenimiento'],\n",
        "\n",
        "    'Experiencias': ['tiempo'],\n",
        "\n",
        "    'Check-in': ['pase', 'pase de abordar', 'abordar', 'abordaje', 'check-in', 'checkin'],\n",
        "\n",
        "    'Otros': ['identificacion', 'accesibilidad', 'discapacidad', 'movilidad', 'discapacitado', 'dicapacitada', 'discapacitados', 'documentacion', 'extras']\n",
        "}"
      ],
      "metadata": {
        "id": "tajiNVgCn5wZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset['category'] = 'General'\n",
        "for idx, row in dataset.iterrows():\n",
        "    text = row['text']\n",
        "\n",
        "    for key, val_list in categories.items():\n",
        "        for val in val_list:\n",
        "            if val.upper() in text.upper():\n",
        "                dataset.at[idx, 'category'] = key\n",
        "                break"
      ],
      "metadata": {
        "id": "hUTowRqxoLlo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Filtro de palabras altisonantes"
      ],
      "metadata": {
        "id": "IG583HRIvnjT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "\n",
        "# Diccionario de palabras altisonantes\n",
        "url = 'https://raw.githubusercontent.com/EddieSharp/Insultos/master/diccionario.txt'\n",
        "\n",
        "response = requests.get(url)\n",
        "content = response.text\n",
        "\n",
        "altisonantes = content.split('\\n')"
      ],
      "metadata": {
        "id": "SkS7SvlOv9wo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for idx, row in dataset.iterrows():\n",
        "    text = row['text']\n",
        "\n",
        "    for palabra in altisonantes:\n",
        "        if palabra.upper() in text.upper():\n",
        "            row['text'].replace(palabra, '***')"
      ],
      "metadata": {
        "id": "4MnNhdbC-vOL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}